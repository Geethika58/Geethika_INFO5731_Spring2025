{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089cc367-0bae-4df8-b555-b6499a86c59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting collection for year: 2000\n",
            "Fetching records for year 2000: 1 to 100...\n",
            "Fetching records for year 2000: 101 to 200...\n",
            "Fetching records for year 2000: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2000: 301 to 400...\n",
            "Fetching records for year 2000: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2001\n",
            "Fetching records for year 2001: 1 to 100...\n",
            "Fetching records for year 2001: 101 to 200...\n",
            "Fetching records for year 2001: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2001: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2002\n",
            "Fetching records for year 2002: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2002: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2002: 201 to 300...\n",
            "Fetching records for year 2002: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2002: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2003\n",
            "Fetching records for year 2003: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2003: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2003: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2003: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2003: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2003: 501 to 600...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2004\n",
            "Fetching records for year 2004: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2004: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2004: 201 to 300...\n",
            "Fetching records for year 2004: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2005\n",
            "Fetching records for year 2005: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2005: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2006\n",
            "Fetching records for year 2006: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2006: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2006: 201 to 300...\n",
            "Fetching records for year 2006: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2006: 401 to 500...\n",
            "Fetching records for year 2006: 501 to 600...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2006: 601 to 700...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2006: 701 to 800...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2007\n",
            "Fetching records for year 2007: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2008\n",
            "Fetching records for year 2008: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2008: 101 to 200...\n",
            "Fetching records for year 2008: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2008: 301 to 400...\n",
            "Fetching records for year 2008: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2008: 501 to 600...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2009\n",
            "Fetching records for year 2009: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2009: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2009: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2009: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2010\n",
            "Fetching records for year 2010: 1 to 100...\n",
            "Fetching records for year 2010: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2011\n",
            "Fetching records for year 2011: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2011: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2011: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2012\n",
            "Fetching records for year 2012: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2012: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2012: 201 to 300...\n",
            "Fetching records for year 2012: 301 to 400...\n",
            "Fetching records for year 2012: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2012: 501 to 600...\n",
            "Fetching records for year 2012: 601 to 700...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2012: 701 to 800...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2012: 801 to 900...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2012: 901 to 1000...\n",
            "Fetching records for year 2012: 1001 to 1100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Error: Received status code 400 for offset 1000 in year 2012. Moving to next year.\n",
            "\n",
            "Starting collection for year: 2013\n",
            "Fetching records for year 2013: 1 to 100...\n",
            "Fetching records for year 2013: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2013: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2013: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2013: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2014\n",
            "Fetching records for year 2014: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2014: 101 to 200...\n",
            "Fetching records for year 2014: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2015\n",
            "Fetching records for year 2015: 1 to 100...\n",
            "Fetching records for year 2015: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2015: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2015: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2015: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2016\n",
            "Fetching records for year 2016: 1 to 100...\n",
            "Fetching records for year 2016: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2016: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2016: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2016: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2016: 501 to 600...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2017\n",
            "Fetching records for year 2017: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2017: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Fetching records for year 2017: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2017: 301 to 400...\n",
            "Fetching records for year 2017: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2018\n",
            "Fetching records for year 2018: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2018: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2018: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2019\n",
            "Fetching records for year 2019: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2019: 101 to 200...\n",
            "Fetching records for year 2019: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2019: 301 to 400...\n",
            "Fetching records for year 2019: 401 to 500...\n",
            "Fetching records for year 2019: 501 to 600...\n",
            "Fetching records for year 2019: 601 to 700...\n",
            "Fetching records for year 2019: 701 to 800...\n",
            "Fetching records for year 2019: 801 to 900...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2019: 901 to 1000...\n",
            "Fetching records for year 2019: 1001 to 1100...\n",
            "Error: Received status code 400 for offset 1000 in year 2019. Moving to next year.\n",
            "\n",
            "Starting collection for year: 2020\n",
            "Fetching records for year 2020: 1 to 100...\n",
            "Fetching records for year 2020: 101 to 200...\n",
            "Fetching records for year 2020: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2020: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2020: 401 to 500...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2020: 501 to 600...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2020: 601 to 700...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2021\n",
            "Fetching records for year 2021: 1 to 100...\n",
            "Fetching records for year 2021: 101 to 200...\n",
            "Fetching records for year 2021: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2022\n",
            "Fetching records for year 2022: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2022: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2022: 201 to 300...\n",
            "Fetching records for year 2022: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2023\n",
            "Fetching records for year 2023: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2023: 101 to 200...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Starting collection for year: 2024\n",
            "Fetching records for year 2024: 1 to 100...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Fetching records for year 2024: 101 to 200...\n",
            "Fetching records for year 2024: 201 to 300...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Fetching records for year 2024: 301 to 400...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Fetching records for year 2024: 401 to 500...\n",
            "Fetching records for year 2024: 501 to 600...\n",
            "Received 429 (Too Many Requests). Backing off for 5 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 10 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 20 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 40 seconds.\n",
            "Received 429 (Too Many Requests). Backing off for 80 seconds.\n",
            "Error: Received status code 429. Stopping.\n",
            "\n",
            "Data collection complete. 9600 records saved to semantic_scholar_abstracts.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Base parameters\n",
        "query_base = \"machine learning\"  # base query string\n",
        "fields = \"paperId,title,abstract,year\"  # fields to retrieve\n",
        "limit = 100  # records per request\n",
        "max_results_needed = 10000  # total records to collect\n",
        "base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Define a range of years to split the query.\n",
        "years = list(range(2000, 2025))\n",
        "\n",
        "csv_filename = \"semantic_scholar_abstracts.csv\"\n",
        "\n",
        "# Function to perform GET requests with exponential backoff for 429 errors\n",
        "def get_with_backoff(url, params, max_retries=5):\n",
        "    backoff_time = 5  # initial backoff in seconds\n",
        "    for attempt in range(max_retries):\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 429:\n",
        "            print(f\"Received 429 (Too Many Requests). Backing off for {backoff_time} seconds.\")\n",
        "            time.sleep(backoff_time)\n",
        "            backoff_time *= 2  # exponential backoff\n",
        "        else:\n",
        "            return response\n",
        "    return response  # return the last response if all retries fail\n",
        "global_count = 0  # count of total papers collected\n",
        "\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Write header row\n",
        "    writer.writerow([\"paperId\", \"title\", \"abstract\", \"year\"])\n",
        "\n",
        "    # Loop over each year to split the query\n",
        "    for year in years:\n",
        "        offset = 0\n",
        "        # Create a query that includes the publication year.\n",
        "        # (This assumes that appending the year helps narrow the results.)\n",
        "        query = f\"{query_base} {year}\"\n",
        "        print(f\"\\nStarting collection for year: {year}\")\n",
        "\n",
        "        while True:\n",
        "            if global_count >= max_results_needed:\n",
        "                break  # Stop if we've reached the target\n",
        "\n",
        "            params = {\n",
        "                \"query\": query,\n",
        "                \"offset\": offset,\n",
        "                \"limit\": limit,\n",
        "                \"fields\": fields\n",
        "            }\n",
        "\n",
        "            print(f\"Fetching records for year {year}: {offset+1} to {offset+limit}...\")\n",
        "            response = get_with_backoff(base_url, params)\n",
        "\n",
        "            # Check for 400 error\n",
        "            if response.status_code == 400:\n",
        "                print(f\"Error: Received status code {response.status_code} for offset {offset} in year {year}. Moving to next year.\")\n",
        "                break\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: Received status code {response.status_code}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            data = response.json()\n",
        "            papers = data.get(\"data\", [])\n",
        "            if not papers:\n",
        "                print(f\"No more papers returned for year {year}.\")\n",
        "                break\n",
        "\n",
        "            # Write each paper's data to CSV\n",
        "            for paper in papers:\n",
        "                writer.writerow([\n",
        "                    paper.get(\"paperId\", \"\"),\n",
        "                    paper.get(\"title\", \"\"),\n",
        "                    paper.get(\"abstract\", \"\"),\n",
        "                    paper.get(\"year\", \"\")\n",
        "                ])\n",
        "                global_count += 1\n",
        "                if global_count >= max_results_needed:\n",
        "                    break  # Stop once target is reached\n",
        "\n",
        "            offset += limit  # update offset for next page\n",
        "            # Brief pause to help avoid rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "        if global_count >= max_results_needed:\n",
        "            break  # Stop processing further years if 10,000 records are collected\n",
        "print(f\"\\nData collection complete. {global_count} records saved to {csv_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94f5c69-5f3e-4458-c2c8-e32b4da0e5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Cleaning a Sample Abstract ---\n",
            "=== Original Text ===\n",
            "This article surveys the contents of the workshop Post-Processing in Machine Learning and Data Mining: Interpretation, Visualization, Integration, and Related Topics within KDD-2000: The Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Boston, MA, USA, 20-23 August 2000. The corresponding web site is on www.acm.org/sigkdd/kdd2000 First, this survey paper introduces the state of the art of the workshop topics, emphasizing that postprocessing forms a significant component in Knowledge Discovery in Databases (KDD). Next, the article brings up a report on the contents, analysis, discussion, and other aspects regarding this workshop. Afterwards, we survey all the workshop papers. They can be found at (and downloaded from) www.cas.mcmaster.ca/~bruha/kdd2000/kddrep.html The authors of this report worked as the organizers of the workshop; the programme committee was formed by additional three researches in this field.\n",
            "\n",
            "=== After Removing Special Characters and Punctuation ===\n",
            "This article surveys the contents of the workshop PostProcessing in Machine Learning and Data Mining Interpretation Visualization Integration and Related Topics within KDD2000 The Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Boston MA USA 2023 August 2000 The corresponding web site is on wwwacmorgsigkddkdd2000 First this survey paper introduces the state of the art of the workshop topics emphasizing that postprocessing forms a significant component in Knowledge Discovery in Databases KDD Next the article brings up a report on the contents analysis discussion and other aspects regarding this workshop Afterwards we survey all the workshop papers They can be found at and downloaded from wwwcasmcmastercabruhakdd2000kddrephtml The authors of this report worked as the organizers of the workshop the programme committee was formed by additional three researches in this field\n",
            "\n",
            "=== After Removing Numbers ===\n",
            "This article surveys the contents of the workshop PostProcessing in Machine Learning and Data Mining Interpretation Visualization Integration and Related Topics within KDD The Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Boston MA USA  August  The corresponding web site is on wwwacmorgsigkddkdd First this survey paper introduces the state of the art of the workshop topics emphasizing that postprocessing forms a significant component in Knowledge Discovery in Databases KDD Next the article brings up a report on the contents analysis discussion and other aspects regarding this workshop Afterwards we survey all the workshop papers They can be found at and downloaded from wwwcasmcmastercabruhakddkddrephtml The authors of this report worked as the organizers of the workshop the programme committee was formed by additional three researches in this field\n",
            "\n",
            "=== After Converting to Lowercase ===\n",
            "this article surveys the contents of the workshop postprocessing in machine learning and data mining interpretation visualization integration and related topics within kdd the sixth acm sigkdd international conference on knowledge discovery and data mining boston ma usa  august  the corresponding web site is on wwwacmorgsigkddkdd first this survey paper introduces the state of the art of the workshop topics emphasizing that postprocessing forms a significant component in knowledge discovery in databases kdd next the article brings up a report on the contents analysis discussion and other aspects regarding this workshop afterwards we survey all the workshop papers they can be found at and downloaded from wwwcasmcmastercabruhakddkddrephtml the authors of this report worked as the organizers of the workshop the programme committee was formed by additional three researches in this field\n",
            "\n",
            "=== After Removing Stopwords ===\n",
            "article surveys contents workshop postprocessing machine learning data mining interpretation visualization integration related topics within kdd sixth acm sigkdd international conference knowledge discovery data mining boston usa august corresponding web site wwwacmorgsigkddkdd first survey paper introduces state art workshop topics emphasizing postprocessing forms significant component knowledge discovery databases kdd next article brings report contents analysis discussion aspects regarding workshop afterwards survey workshop papers found downloaded wwwcasmcmastercabruhakddkddrephtml authors report worked organizers workshop programme committee formed additional three researches field\n",
            "\n",
            "=== After Stemming ===\n",
            "articl survey content workshop postprocess machin learn data mine interpret visual integr relat topic within kdd sixth acm sigkdd intern confer knowledg discoveri data mine boston usa august correspond web site wwwacmorgsigkddkdd first survey paper introduc state art workshop topic emphas postprocess form signific compon knowledg discoveri databas kdd next articl bring report content analysi discuss aspect regard workshop afterward survey workshop paper found download wwwcasmcmastercabruhakddkddrephtml author report work organ workshop programm committe form addit three research field\n",
            "\n",
            "=== After Lemmatization ===\n",
            "articl survey content workshop postprocess machin learn data mine interpret visual integr relat topic within kdd sixth acm sigkdd intern confer knowledg discoveri data mine boston usa august correspond web site wwwacmorgsigkddkdd first survey paper introduc state art workshop topic emphas postprocess form signific compon knowledg discoveri databas kdd next articl bring report content analysi discus aspect regard workshop afterward survey workshop paper found download wwwcasmcmastercabruhakddkddrephtml author report work organ workshop programm committe form addit three research field\n",
            "\n",
            "Cleaned data has been saved to 'semantic_scholar_abstracts_clean.csv'\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"semantic_scholar_abstracts.csv\")\n",
        "\n",
        "# Define a function that cleans a text and prints intermediate outputs for demonstration.\n",
        "def clean_text_verbose(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text  # if text is NaN or not a string, return it unchanged.\n",
        "\n",
        "    print(\"=== Original Text ===\")\n",
        "    print(text)\n",
        "\n",
        "    # (1) Remove noise: special characters and punctuation\n",
        "    text_no_special = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    print(\"\\n=== After Removing Special Characters and Punctuation ===\")\n",
        "    print(text_no_special)\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text_no_numbers = re.sub(r'\\d+', '', text_no_special)\n",
        "    print(\"\\n=== After Removing Numbers ===\")\n",
        "    print(text_no_numbers)\n",
        "\n",
        "    # (4) Lowercase all texts\n",
        "    text_lower = text_no_numbers.lower()\n",
        "    print(\"\\n=== After Converting to Lowercase ===\")\n",
        "    print(text_lower)\n",
        "\n",
        "    # (3) Remove stopwords using the stopwords list\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text_lower.split()\n",
        "    words_no_stop = [word for word in words if word not in stop_words]\n",
        "    text_no_stop = ' '.join(words_no_stop)\n",
        "    print(\"\\n=== After Removing Stopwords ===\")\n",
        "    print(text_no_stop)\n",
        "\n",
        "    # (5) Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words_stemmed = [stemmer.stem(word) for word in text_no_stop.split()]\n",
        "    text_stemmed = ' '.join(words_stemmed)\n",
        "    print(\"\\n=== After Stemming ===\")\n",
        "    print(text_stemmed)\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words_lemmatized = [lemmatizer.lemmatize(word) for word in text_stemmed.split()]\n",
        "    text_lemmatized = ' '.join(words_lemmatized)\n",
        "    print(\"\\n=== After Lemmatization ===\")\n",
        "    print(text_lemmatized)\n",
        "\n",
        "    return text_lemmatized\n",
        "\n",
        "# Demonstrate the cleaning steps on one sample abstract\n",
        "sample_abstract = df['abstract'].dropna().iloc[0]\n",
        "print(\"\\n\\n--- Cleaning a Sample Abstract ---\")\n",
        "cleaned_sample = clean_text_verbose(sample_abstract)\n",
        "\n",
        "# Now define a streamlined cleaning function to apply to the entire dataset\n",
        "def final_clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in text.split() if word not in stop_words]\n",
        "    text = ' '.join(words)\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in text.split()]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the cleaning to the \"abstract\" column and store results in a new column \"clean_abstract\"\n",
        "df['clean_abstract'] = df['abstract'].apply(final_clean_text)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_filename = \"semantic_scholar_abstracts_clean.csv\"\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\nCleaned data has been saved to '{output_filename}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db43adb-b151-4db4-8386-e6d6cf8de260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Part-of-Speech (POS) Tagging and Count Calculation ===\n",
            "Total Nouns:      813783\n",
            "Total Verbs:      121065\n",
            "Total Adjectives: 105515\n",
            "Total Adverbs:    12023\n",
            "\n",
            "=== Constituency Parsing and Dependency Parsing ===\n",
            "\n",
            "Sample Sentence for Parsing:\n",
            "articl survey content workshop postprocess machin learn data mine interpret visual integr relat topic within kdd sixth acm sigkdd intern confer knowledg discoveri data mine boston usa august correspond web site wwwacmorgsigkddkdd first survey paper introduc state art workshop topic emphas postprocess form signific compon knowledg discoveri databas kdd next articl bring report content analysi discus aspect regard workshop afterward survey workshop paper found download wwwcasmcmastercabruhakddkddrephtml author report work organ workshop programm committe form addit three research field\n",
            "\n",
            "Constituency Parse Tree (via NLTK RegexpParser):\n",
            "(S\n",
            "  (NP\n",
            "    articl/NNP\n",
            "    survey/NN\n",
            "    content/NN\n",
            "    workshop/NNP\n",
            "    postprocess/NN\n",
            "    machin/NN)\n",
            "  (VP learn/VBP (NP data/NNS))\n",
            "  mine/PRP\n",
            "  (VP\n",
            "    interpret/VBP\n",
            "    (NP visual/JJ integr/NNP relat/NNP topic/NN)\n",
            "    (PP within/IN (NP kdd/NNP))\n",
            "    (NP\n",
            "      sixth/JJ\n",
            "      acm/NN\n",
            "      sigkdd/NNP\n",
            "      intern/NNP\n",
            "      confer/NNP\n",
            "      knowledg/NNP\n",
            "      discoveri/NNP\n",
            "      data/NNP\n",
            "      mine/NNP\n",
            "      boston/NNP\n",
            "      usa/NNP\n",
            "      august/NNP\n",
            "      correspond/NNP\n",
            "      web/NN\n",
            "      site/NN))\n",
            "  (VP\n",
            "    wwwacmorgsigkddkdd/VBD\n",
            "    (NP\n",
            "      first/JJ\n",
            "      survey/NN\n",
            "      paper/NN\n",
            "      introduc/NNP\n",
            "      state/NN\n",
            "      art/NN\n",
            "      workshop/NN\n",
            "      topic/NN\n",
            "      emphas/NNP)\n",
            "    (NP\n",
            "      postprocess/JJ\n",
            "      form/NN\n",
            "      signific/NN\n",
            "      compon/NN\n",
            "      knowledg/NNP\n",
            "      discoveri/NNP\n",
            "      databas/NNP\n",
            "      kdd/NNP)\n",
            "    (NP next/JJ articl/NNP))\n",
            "  (VP\n",
            "    bring/VBP\n",
            "    (NP\n",
            "      report/NN\n",
            "      content/NN\n",
            "      analysi/NNP\n",
            "      discus/NN\n",
            "      aspect/NNP\n",
            "      regard/NN\n",
            "      workshop/NN))\n",
            "  afterward/RB\n",
            "  (NP survey/NN workshop/NN paper/NN)\n",
            "  (VP\n",
            "    found/VBD\n",
            "    (NP\n",
            "      download/NN\n",
            "      wwwcasmcmastercabruhakddkddrephtml/NNP\n",
            "      author/NN\n",
            "      report/NN\n",
            "      work/NN\n",
            "      organ/NN\n",
            "      workshop/NN\n",
            "      programm/NNP\n",
            "      committe/NN\n",
            "      form/NN\n",
            "      addit/NN))\n",
            "  three/CD\n",
            "  (NP research/NN field/NN))\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                               S                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "    ___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________|___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________            \n",
            "   |          |          |                              |                                                       |                                                                                                                      VP                                                                                                                                                                                                                            |                                                                                                                                                             |                                                                |                                                                             |                                                                                |          \n",
            "   |          |          |                              |                                                       |                  ____________________________________________________________________________________________________|_______________________________________________                                                                                                                                                                              |                                                                                                                                                             |                                                                |                                                                             |                                                                                |           \n",
            "   |          |          |                              |                                                       VP                |                     |                                    PP                                                                                        |                                                                                                                                                                             VP                                                                                                                                                            VP                                                               |                                                                             VP                                                                               |          \n",
            "   |          |          |                              |                                                 ______|_____            |                     |                              ______|_____                                                                                    |                                                                                  ___________________________________________________________________________________________|___________________________________________________________________________________________________________                   _______________________________|__________                                                      |                    _________________________________________________________|_______                                                                         |           \n",
            "   |          |          |                              NP                                               |            NP          |                     NP                            |            NP                                                                                  NP                                                                                |                                                     NP                                                                                  NP                                                            NP                |                                          NP                                                    NP                  |                                                                 NP                                                                       NP         \n",
            "   |          |          |          ____________________|______________________________________          |            |           |            _________|___________________          |            |        ___________________________________________________________________________|_____________________________________________________________________            |             ________________________________________|_____________________________________              ________________________________|_______________________________________________         _____|______           |          ________________________________|_______________________________            __________|_________          |           ______________________________________________________|_____________________________________________________            _______|_____      \n",
            "mine/PRP afterward/RB three/CD articl/NNP survey/NN content/NN workshop/NNP postprocess/NN machin/NN learn/VBP     data/NNS interpret/VBP visual/JJ integr/NNP relat/NNP topic/NN within/IN     kdd/NNP sixth/JJ acm/NN sigkdd/NNP intern/NNP confer/NNP knowledg/NNP discoveri/NNP data/NNP mine/NNP boston/NNP usa/NNP august/NNP correspond/NNP web/NN site/NN wwwacmorgsigkddk first/JJ survey/NN paper/NN introduc/NNP state/NN art/NN workshop/NN topic/NN emphas/NNP postprocess/JJ form/NN signific/NN compon/NN knowledg/NNP discoveri/NNP databas/NNP kdd/NNP next/JJ     articl/NNP bring/VBP report/NN content/NN analysi/NNP discus/NN aspect/NNP regard/NN workshop/NN survey/NN workshop/NN paper/NN found/VBD download/NN wwwcasmcmasterca author/NN report/NN work/NN organ/NN workshop/NN programm/NNP committe/NN form/NN addit/NN research/NN     field/NN\n",
            "                                                                                                                                                                                                                                                                                                                                                                       dd/VBD                                                                                                                                                                                                                                                                                                                                                             bruhakddkddrepht                                                                                                                    \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ml/NNP                                                                                                                         \n",
            "\n",
            "\n",
            "Dependency Parse (token, dependency relation, head):\n",
            "articl          compound   content\n",
            "survey          compound   content\n",
            "content         compound   machin\n",
            "workshop        compound   machin\n",
            "postprocess     compound   machin\n",
            "machin          nsubj      learn\n",
            "learn           nsubj      wwwacmorgsigkddkdd\n",
            "data            compound   mine\n",
            "mine            nsubj      interpret\n",
            "interpret       ccomp      learn\n",
            "visual          amod       integr\n",
            "integr          compound   relat\n",
            "relat           compound   topic\n",
            "topic           dobj       interpret\n",
            "within          prep       interpret\n",
            "kdd             nmod       acm\n",
            "sixth           amod       acm\n",
            "acm             compound   sigkdd\n",
            "sigkdd          compound   boston\n",
            "intern          compound   confer\n",
            "confer          compound   knowledg\n",
            "knowledg        compound   mine\n",
            "discoveri       compound   data\n",
            "data            compound   mine\n",
            "mine            compound   boston\n",
            "boston          compound   site\n",
            "usa             compound   site\n",
            "august          compound   site\n",
            "correspond      compound   site\n",
            "web             compound   site\n",
            "site            pobj       within\n",
            "wwwacmorgsigkddkdd ROOT       wwwacmorgsigkddkdd\n",
            "first           amod       topic\n",
            "survey          compound   paper\n",
            "paper           compound   topic\n",
            "introduc        amod       topic\n",
            "state           compound   workshop\n",
            "art             compound   workshop\n",
            "workshop        compound   topic\n",
            "topic           dobj       wwwacmorgsigkddkdd\n",
            "emphas          nmod       form\n",
            "postprocess     amod       form\n",
            "form            appos      topic\n",
            "signific        amod       compon\n",
            "compon          compound   knowledg\n",
            "knowledg        compound   databas\n",
            "discoveri       compound   databas\n",
            "databas         dobj       wwwacmorgsigkddkdd\n",
            "kdd             compound   articl\n",
            "next            amod       articl\n",
            "articl          nsubj      bring\n",
            "bring           ccomp      wwwacmorgsigkddkdd\n",
            "report          compound   aspect\n",
            "content         compound   aspect\n",
            "analysi         compound   aspect\n",
            "discus          compound   aspect\n",
            "aspect          compound   workshop\n",
            "regard          compound   workshop\n",
            "workshop        nsubj      found\n",
            "afterward       advmod     workshop\n",
            "survey          compound   paper\n",
            "workshop        compound   paper\n",
            "paper           appos      workshop\n",
            "found           advcl      bring\n",
            "download        compound   wwwcasmcmastercabruhakddkddrephtml\n",
            "wwwcasmcmastercabruhakddkddrephtml compound   author\n",
            "author          compound   report\n",
            "report          compound   workshop\n",
            "work            compound   workshop\n",
            "organ           compound   workshop\n",
            "workshop        nsubj      addit\n",
            "programm        compound   form\n",
            "committe        compound   form\n",
            "form            appos      workshop\n",
            "addit           ccomp      found\n",
            "three           nummod     field\n",
            "research        compound   field\n",
            "field           dobj       bring\n",
            "\n",
            "Explanation:\n",
            "1. Constituency Parse Tree: The tree above is generated using a simple grammar to chunk the sentence into constituents like NP (noun phrase), VP (verb phrase), and PP (prepositional phrase).\n",
            "   It provides a rough hierarchical structure of the sentence, showing how words combine into larger phrases.\n",
            "2. Dependency Parse: Each token is printed along with its dependency relation and head token, illustrating the grammatical relationships (e.g., subject, object) between words.\n",
            "\n",
            "=== Named Entity Recognition (NER) ===\n",
            "Entity Counts:\n",
            "PERSON: 21865\n",
            "ORG: 15754\n",
            "GPE: 5812\n",
            "PRODUCT: 573\n",
            "DATE: 3951\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "\n",
        "# Ensure required NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 1: Load spaCy's English model\n",
        "# -------------------------------------------\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Step 2: Load the clean abstracts CSV file\n",
        "# -------------------------------------------\n",
        "df = pd.read_csv(\"semantic_scholar_abstracts_clean.csv\")\n",
        "clean_texts = df['clean_abstract'].dropna()\n",
        "\n",
        "# =============================================================================\n",
        "# (1) Parts of Speech (POS) Tagging and Count Calculation\n",
        "# =============================================================================\n",
        "print(\"=== Part-of-Speech (POS) Tagging and Count Calculation ===\")\n",
        "\n",
        "total_noun = 0\n",
        "total_verb = 0\n",
        "total_adj  = 0\n",
        "total_adv  = 0\n",
        "\n",
        "for text in clean_texts:\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "            total_noun += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            total_verb += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            total_adj += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            total_adv += 1\n",
        "\n",
        "print(f\"Total Nouns:      {total_noun}\")\n",
        "print(f\"Total Verbs:      {total_verb}\")\n",
        "print(f\"Total Adjectives: {total_adj}\")\n",
        "print(f\"Total Adverbs:    {total_adv}\")\n",
        "\n",
        "# =============================================================================\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "# =============================================================================\n",
        "print(\"\\n=== Constituency Parsing and Dependency Parsing ===\")\n",
        "\n",
        "# For demonstration, select one sample sentence from the first clean abstract.\n",
        "sample_text = clean_texts.iloc[0]\n",
        "doc_sample = nlp(sample_text)\n",
        "sample_sentence = list(doc_sample.sents)[0]\n",
        "\n",
        "print(\"\\nSample Sentence for Parsing:\")\n",
        "print(sample_sentence.text)\n",
        "\n",
        "# --- Constituency Parsing using NLTK's RegexpParser ---\n",
        "# Convert the sample sentence into a list of tuples using spaCy tokens.\n",
        "tokens = [(token.text, token.tag_) for token in sample_sentence]\n",
        "\n",
        "# Define a simple grammar for chunking into noun phrases (NP), verb phrases (VP), and prepositional phrases (PP)\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT>?<JJ.*>*<NN.*>+}   # Noun Phrase: optional determiner, adjectives, and one or more nouns\n",
        "  PP: {<IN><NP>}             # Prepositional Phrase: preposition followed by a noun phrase\n",
        "  VP: {<VB.*><NP|PP>*}       # Verb Phrase: verb followed by optional noun or prepositional phrases\n",
        "\"\"\"\n",
        "\n",
        "# Create the RegexpParser and parse the tokens\n",
        "constituency_parser = RegexpParser(grammar)\n",
        "constituency_tree = constituency_parser.parse(tokens)\n",
        "\n",
        "print(\"\\nConstituency Parse Tree (via NLTK RegexpParser):\")\n",
        "print(constituency_tree)\n",
        "constituency_tree.pretty_print()\n",
        "\n",
        "# --- Dependency Parsing using spaCy ---\n",
        "print(\"\\nDependency Parse (token, dependency relation, head):\")\n",
        "for token in sample_sentence:\n",
        "    print(f\"{token.text:15} {token.dep_:10} {token.head.text}\")\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"1. Constituency Parse Tree: The tree above is generated using a simple grammar to chunk the sentence into constituents like NP (noun phrase), VP (verb phrase), and PP (prepositional phrase).\")\n",
        "print(\"   It provides a rough hierarchical structure of the sentence, showing how words combine into larger phrases.\")\n",
        "print(\"2. Dependency Parse: Each token is printed along with its dependency relation and head token, illustrating the grammatical relationships (e.g., subject, object) between words.\")\n",
        "\n",
        "# =============================================================================\n",
        "# (3) Named Entity Recognition (NER)\n",
        "# =============================================================================\n",
        "print(\"\\n=== Named Entity Recognition (NER) ===\")\n",
        "# Define the entity labels of interest: PERSON, ORG, GPE (locations), PRODUCT, and DATE.\n",
        "entity_labels = [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"DATE\"]\n",
        "entity_counter = Counter()\n",
        "\n",
        "for text in clean_texts:\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_labels:\n",
        "            entity_counter[ent.label_] += 1\n",
        "\n",
        "print(\"Entity Counts:\")\n",
        "for label in entity_labels:\n",
        "    print(f\"{label}: {entity_counter[label]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests-html nest_asyncio pandas nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wk9EXSEmcjju",
        "outputId": "f616fe0e-7275-4c40-8248-bfb51ef51372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->requests-html) (4.13.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html) (5.3.1)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->requests-html) (2.6)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.8.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 fake-useragent-2.0.3 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              },
              "id": "d7a202cea8f84ab9b49c153c9680f2d8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"lxml[html_clean]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QarXbOPyeGw6",
        "outputId": "1982ed7e-16ab-4308-f31c-a43d991b6ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Collecting lxml_html_clean (from lxml[html_clean])\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "\n",
        "def fetch_products_from_page(page_num):\n",
        "    \"\"\"\n",
        "    Fetch and parse a single GitHub Marketplace Actions page.\n",
        "\n",
        "    Args:\n",
        "        page_num (int): The page number to fetch.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing:\n",
        "              - Product Name\n",
        "              - Description\n",
        "              - URL (absolute)\n",
        "              - Page Number\n",
        "    \"\"\"\n",
        "    url = f\"https://github.com/marketplace?type=actions&page={page_num}\"\n",
        "\n",
        "    # Set headers to mimic a real browser request\n",
        "    headers = {\n",
        "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                       \"Chrome/115.0.0.0 Safari/537.36\"),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Referer\": \"https://github.com/\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Page {page_num}: Error fetching page (Status code: {response.status_code}).\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find product cards using the data-testid attribute (as per inspected HTML)\n",
        "    product_cards = soup.find_all(\"div\", attrs={\"data-testid\": \"marketplace-item\"})\n",
        "    products = []\n",
        "\n",
        "    for card in product_cards:\n",
        "        # Extract product name and URL from the <h3> element containing the <a> tag\n",
        "        h3_tag = card.find(\"h3\")\n",
        "        if h3_tag:\n",
        "            a_tag = h3_tag.find(\"a\", href=True)\n",
        "            if a_tag:\n",
        "                product_name = a_tag.get_text(strip=True)\n",
        "                # Construct absolute URL (assuming href is a relative link)\n",
        "                product_url = \"https://github.com\" + a_tag.get(\"href\", \"\")\n",
        "            else:\n",
        "                product_name = \"N/A\"\n",
        "                product_url = \"N/A\"\n",
        "        else:\n",
        "            product_name = \"N/A\"\n",
        "            product_url = \"N/A\"\n",
        "\n",
        "        # Extract the product description from the <p> tag with known classes\n",
        "        p_tag = card.find(\"p\", class_=\"mt-1 mb-0 text-small fgColor-muted line-clamp-2\")\n",
        "        description = p_tag.get_text(strip=True) if p_tag else \"N/A\"\n",
        "\n",
        "        products.append({\n",
        "            \"Product Name\": product_name,\n",
        "            \"Description\": description,\n",
        "            \"URL\": product_url,\n",
        "            \"Page Number\": page_num\n",
        "        })\n",
        "\n",
        "    return products\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Loops through pages 1 to 500, scrapes product data from each page,\n",
        "    and saves the combined results to a CSV file.\n",
        "    \"\"\"\n",
        "    all_products = []\n",
        "    max_pages = 500  # Loop through 500 pages regardless of product count\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        print(f\"Loading page {page}...\")\n",
        "        products = fetch_products_from_page(page)\n",
        "        if products:\n",
        "            print(f\"Page {page} complete. Extracted {len(products)} products.\")\n",
        "            all_products.extend(products)\n",
        "        else:\n",
        "            print(f\"Skipping page {page} due to error.\")\n",
        "\n",
        "        # Pause randomly between 1 and 3 seconds to avoid server overload\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    # Save all extracted products to a CSV file\n",
        "    csv_filename = \"scraped_products.csv\"\n",
        "    fieldnames = [\"Product Name\", \"Description\", \"URL\", \"Page Number\"]\n",
        "\n",
        "    try:\n",
        "        with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for product in all_products:\n",
        "                writer.writerow(product)\n",
        "        print(f\"Product data saved to '{csv_filename}'.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error saving CSV:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL_UupVUfYTf",
        "outputId": "d6251d3f-b07a-46a0-fdfd-02641de18755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading page 1...\n",
            "Page 1 complete. Extracted 20 products.\n",
            "Loading page 2...\n",
            "Page 2 complete. Extracted 20 products.\n",
            "Loading page 3...\n",
            "Page 3 complete. Extracted 20 products.\n",
            "Loading page 4...\n",
            "Page 4 complete. Extracted 20 products.\n",
            "Loading page 5...\n",
            "Page 5 complete. Extracted 20 products.\n",
            "Loading page 6...\n",
            "Page 6 complete. Extracted 20 products.\n",
            "Loading page 7...\n",
            "Page 7 complete. Extracted 20 products.\n",
            "Loading page 8...\n",
            "Skipping page 8 due to error.\n",
            "Loading page 9...\n",
            "Page 9 complete. Extracted 20 products.\n",
            "Loading page 10...\n",
            "Skipping page 10 due to error.\n",
            "Loading page 11...\n",
            "Page 11 complete. Extracted 20 products.\n",
            "Loading page 12...\n",
            "Page 12 complete. Extracted 20 products.\n",
            "Loading page 13...\n",
            "Page 13 complete. Extracted 20 products.\n",
            "Loading page 14...\n",
            "Page 14 complete. Extracted 20 products.\n",
            "Loading page 15...\n",
            "Page 15 complete. Extracted 20 products.\n",
            "Loading page 16...\n",
            "Skipping page 16 due to error.\n",
            "Loading page 17...\n",
            "Page 17 complete. Extracted 20 products.\n",
            "Loading page 18...\n",
            "Page 18 complete. Extracted 20 products.\n",
            "Loading page 19...\n",
            "Page 19 complete. Extracted 20 products.\n",
            "Loading page 20...\n",
            "Page 20 complete. Extracted 20 products.\n",
            "Loading page 21...\n",
            "Page 21 complete. Extracted 20 products.\n",
            "Loading page 22...\n",
            "Page 22 complete. Extracted 20 products.\n",
            "Loading page 23...\n",
            "Page 23 complete. Extracted 20 products.\n",
            "Loading page 24...\n",
            "Page 24 complete. Extracted 20 products.\n",
            "Loading page 25...\n",
            "Page 25 complete. Extracted 20 products.\n",
            "Loading page 26...\n",
            "Page 26 complete. Extracted 20 products.\n",
            "Loading page 27...\n",
            "Page 27 complete. Extracted 20 products.\n",
            "Loading page 28...\n",
            "Page 28 complete. Extracted 20 products.\n",
            "Loading page 29...\n",
            "Page 29 complete. Extracted 20 products.\n",
            "Loading page 30...\n",
            "Page 30 complete. Extracted 20 products.\n",
            "Loading page 31...\n",
            "Page 31 complete. Extracted 20 products.\n",
            "Loading page 32...\n",
            "Page 32 complete. Extracted 20 products.\n",
            "Loading page 33...\n",
            "Page 33 complete. Extracted 20 products.\n",
            "Loading page 34...\n",
            "Page 34 complete. Extracted 20 products.\n",
            "Loading page 35...\n",
            "Page 35 complete. Extracted 20 products.\n",
            "Loading page 36...\n",
            "Page 36 complete. Extracted 20 products.\n",
            "Loading page 37...\n",
            "Page 37 complete. Extracted 20 products.\n",
            "Loading page 38...\n",
            "Page 38 complete. Extracted 20 products.\n",
            "Loading page 39...\n",
            "Page 39 complete. Extracted 20 products.\n",
            "Loading page 40...\n",
            "Page 40 complete. Extracted 20 products.\n",
            "Loading page 41...\n",
            "Page 41 complete. Extracted 20 products.\n",
            "Loading page 42...\n",
            "Page 42 complete. Extracted 20 products.\n",
            "Loading page 43...\n",
            "Page 43 complete. Extracted 20 products.\n",
            "Loading page 44...\n",
            "Page 44 complete. Extracted 20 products.\n",
            "Loading page 45...\n",
            "Page 45 complete. Extracted 20 products.\n",
            "Loading page 46...\n",
            "Page 46 complete. Extracted 20 products.\n",
            "Loading page 47...\n",
            "Page 47 complete. Extracted 20 products.\n",
            "Loading page 48...\n",
            "Page 48 complete. Extracted 20 products.\n",
            "Loading page 49...\n",
            "Page 49 complete. Extracted 20 products.\n",
            "Loading page 50...\n",
            "Page 50 complete. Extracted 20 products.\n",
            "Loading page 51...\n",
            "Page 51 complete. Extracted 20 products.\n",
            "Loading page 52...\n",
            "Page 52 complete. Extracted 20 products.\n",
            "Loading page 53...\n",
            "Page 53 complete. Extracted 20 products.\n",
            "Loading page 54...\n",
            "Page 54 complete. Extracted 20 products.\n",
            "Loading page 55...\n",
            "Page 55 complete. Extracted 20 products.\n",
            "Loading page 56...\n",
            "Page 56 complete. Extracted 20 products.\n",
            "Loading page 57...\n",
            "Page 57 complete. Extracted 20 products.\n",
            "Loading page 58...\n",
            "Page 58 complete. Extracted 20 products.\n",
            "Loading page 59...\n",
            "Page 59 complete. Extracted 20 products.\n",
            "Loading page 60...\n",
            "Page 60 complete. Extracted 20 products.\n",
            "Loading page 61...\n",
            "Page 61 complete. Extracted 20 products.\n",
            "Loading page 62...\n",
            "Page 62 complete. Extracted 20 products.\n",
            "Loading page 63...\n",
            "Page 63 complete. Extracted 20 products.\n",
            "Loading page 64...\n",
            "Page 64 complete. Extracted 20 products.\n",
            "Loading page 65...\n",
            "Page 65 complete. Extracted 20 products.\n",
            "Loading page 66...\n",
            "Page 66 complete. Extracted 20 products.\n",
            "Loading page 67...\n",
            "Page 67 complete. Extracted 20 products.\n",
            "Loading page 68...\n",
            "Page 68 complete. Extracted 20 products.\n",
            "Loading page 69...\n",
            "Page 69 complete. Extracted 20 products.\n",
            "Loading page 70...\n",
            "Page 70 complete. Extracted 20 products.\n",
            "Loading page 71...\n",
            "Page 71 complete. Extracted 20 products.\n",
            "Loading page 72...\n",
            "Page 72 complete. Extracted 20 products.\n",
            "Loading page 73...\n",
            "Page 73 complete. Extracted 20 products.\n",
            "Loading page 74...\n",
            "Page 74 complete. Extracted 20 products.\n",
            "Loading page 75...\n",
            "Page 75 complete. Extracted 20 products.\n",
            "Loading page 76...\n",
            "Page 76 complete. Extracted 20 products.\n",
            "Loading page 77...\n",
            "Page 77 complete. Extracted 20 products.\n",
            "Loading page 78...\n",
            "Page 78 complete. Extracted 20 products.\n",
            "Loading page 79...\n",
            "Page 79 complete. Extracted 20 products.\n",
            "Loading page 80...\n",
            "Page 80 complete. Extracted 20 products.\n",
            "Loading page 81...\n",
            "Page 81 complete. Extracted 20 products.\n",
            "Loading page 82...\n",
            "Page 82 complete. Extracted 20 products.\n",
            "Loading page 83...\n",
            "Page 83 complete. Extracted 20 products.\n",
            "Loading page 84...\n",
            "Page 84 complete. Extracted 20 products.\n",
            "Loading page 85...\n",
            "Page 85 complete. Extracted 20 products.\n",
            "Loading page 86...\n",
            "Page 86 complete. Extracted 20 products.\n",
            "Loading page 87...\n",
            "Page 87 complete. Extracted 20 products.\n",
            "Loading page 88...\n",
            "Page 88 complete. Extracted 20 products.\n",
            "Loading page 89...\n",
            "Page 89 complete. Extracted 20 products.\n",
            "Loading page 90...\n",
            "Page 90 complete. Extracted 20 products.\n",
            "Loading page 91...\n",
            "Page 91 complete. Extracted 20 products.\n",
            "Loading page 92...\n",
            "Page 92 complete. Extracted 20 products.\n",
            "Loading page 93...\n",
            "Page 93 complete. Extracted 20 products.\n",
            "Loading page 94...\n",
            "Page 94 complete. Extracted 20 products.\n",
            "Loading page 95...\n",
            "Page 95 complete. Extracted 20 products.\n",
            "Loading page 96...\n",
            "Page 96 complete. Extracted 20 products.\n",
            "Loading page 97...\n",
            "Page 97 complete. Extracted 20 products.\n",
            "Loading page 98...\n",
            "Page 98 complete. Extracted 20 products.\n",
            "Loading page 99...\n",
            "Page 99 complete. Extracted 20 products.\n",
            "Loading page 100...\n",
            "Page 100 complete. Extracted 20 products.\n",
            "Loading page 101...\n",
            "Page 101 complete. Extracted 20 products.\n",
            "Loading page 102...\n",
            "Page 102 complete. Extracted 20 products.\n",
            "Loading page 103...\n",
            "Page 103 complete. Extracted 20 products.\n",
            "Loading page 104...\n",
            "Page 104 complete. Extracted 20 products.\n",
            "Loading page 105...\n",
            "Page 105 complete. Extracted 20 products.\n",
            "Loading page 106...\n",
            "Page 106 complete. Extracted 20 products.\n",
            "Loading page 107...\n",
            "Page 107 complete. Extracted 20 products.\n",
            "Loading page 108...\n",
            "Page 108 complete. Extracted 20 products.\n",
            "Loading page 109...\n",
            "Page 109 complete. Extracted 20 products.\n",
            "Loading page 110...\n",
            "Page 110 complete. Extracted 20 products.\n",
            "Loading page 111...\n",
            "Page 111 complete. Extracted 20 products.\n",
            "Loading page 112...\n",
            "Page 112 complete. Extracted 20 products.\n",
            "Loading page 113...\n",
            "Page 113 complete. Extracted 20 products.\n",
            "Loading page 114...\n",
            "Page 114 complete. Extracted 20 products.\n",
            "Loading page 115...\n",
            "Page 115 complete. Extracted 20 products.\n",
            "Loading page 116...\n",
            "Page 116 complete. Extracted 20 products.\n",
            "Loading page 117...\n",
            "Page 117 complete. Extracted 20 products.\n",
            "Loading page 118...\n",
            "Page 118 complete. Extracted 20 products.\n",
            "Loading page 119...\n",
            "Page 119 complete. Extracted 20 products.\n",
            "Loading page 120...\n",
            "Page 120 complete. Extracted 20 products.\n",
            "Loading page 121...\n",
            "Page 121 complete. Extracted 20 products.\n",
            "Loading page 122...\n",
            "Page 122 complete. Extracted 20 products.\n",
            "Loading page 123...\n",
            "Page 123 complete. Extracted 20 products.\n",
            "Loading page 124...\n",
            "Page 124 complete. Extracted 20 products.\n",
            "Loading page 125...\n",
            "Page 125 complete. Extracted 20 products.\n",
            "Loading page 126...\n",
            "Page 126 complete. Extracted 20 products.\n",
            "Loading page 127...\n",
            "Page 127 complete. Extracted 20 products.\n",
            "Loading page 128...\n",
            "Page 128 complete. Extracted 20 products.\n",
            "Loading page 129...\n",
            "Page 129 complete. Extracted 20 products.\n",
            "Loading page 130...\n",
            "Page 130 complete. Extracted 20 products.\n",
            "Loading page 131...\n",
            "Page 131 complete. Extracted 20 products.\n",
            "Loading page 132...\n",
            "Page 132 complete. Extracted 20 products.\n",
            "Loading page 133...\n",
            "Page 133 complete. Extracted 20 products.\n",
            "Loading page 134...\n",
            "Page 134 complete. Extracted 20 products.\n",
            "Loading page 135...\n",
            "Page 135 complete. Extracted 20 products.\n",
            "Loading page 136...\n",
            "Page 136 complete. Extracted 20 products.\n",
            "Loading page 137...\n",
            "Page 137 complete. Extracted 20 products.\n",
            "Loading page 138...\n",
            "Page 138 complete. Extracted 20 products.\n",
            "Loading page 139...\n",
            "Page 139 complete. Extracted 20 products.\n",
            "Loading page 140...\n",
            "Page 140 complete. Extracted 20 products.\n",
            "Loading page 141...\n",
            "Page 141 complete. Extracted 20 products.\n",
            "Loading page 142...\n",
            "Page 142 complete. Extracted 20 products.\n",
            "Loading page 143...\n",
            "Page 143 complete. Extracted 20 products.\n",
            "Loading page 144...\n",
            "Page 144 complete. Extracted 20 products.\n",
            "Loading page 145...\n",
            "Page 145 complete. Extracted 20 products.\n",
            "Loading page 146...\n",
            "Page 146 complete. Extracted 20 products.\n",
            "Loading page 147...\n",
            "Page 147 complete. Extracted 20 products.\n",
            "Loading page 148...\n",
            "Page 148 complete. Extracted 20 products.\n",
            "Loading page 149...\n",
            "Page 149 complete. Extracted 20 products.\n",
            "Loading page 150...\n",
            "Page 150 complete. Extracted 20 products.\n",
            "Loading page 151...\n",
            "Page 151 complete. Extracted 20 products.\n",
            "Loading page 152...\n",
            "Page 152 complete. Extracted 20 products.\n",
            "Loading page 153...\n",
            "Page 153 complete. Extracted 20 products.\n",
            "Loading page 154...\n",
            "Page 154 complete. Extracted 20 products.\n",
            "Loading page 155...\n",
            "Page 155 complete. Extracted 20 products.\n",
            "Loading page 156...\n",
            "Page 156 complete. Extracted 20 products.\n",
            "Loading page 157...\n",
            "Page 157 complete. Extracted 20 products.\n",
            "Loading page 158...\n",
            "Page 158 complete. Extracted 20 products.\n",
            "Loading page 159...\n",
            "Page 159 complete. Extracted 20 products.\n",
            "Loading page 160...\n",
            "Page 160 complete. Extracted 20 products.\n",
            "Loading page 161...\n",
            "Page 161 complete. Extracted 20 products.\n",
            "Loading page 162...\n",
            "Page 162 complete. Extracted 20 products.\n",
            "Loading page 163...\n",
            "Page 163 complete. Extracted 20 products.\n",
            "Loading page 164...\n",
            "Page 164 complete. Extracted 20 products.\n",
            "Loading page 165...\n",
            "Page 165 complete. Extracted 20 products.\n",
            "Loading page 166...\n",
            "Skipping page 166 due to error.\n",
            "Loading page 167...\n",
            "Page 167 complete. Extracted 20 products.\n",
            "Loading page 168...\n",
            "Page 168 complete. Extracted 20 products.\n",
            "Loading page 169...\n",
            "Page 169 complete. Extracted 20 products.\n",
            "Loading page 170...\n",
            "Page 170 complete. Extracted 20 products.\n",
            "Loading page 171...\n",
            "Page 171 complete. Extracted 20 products.\n",
            "Loading page 172...\n",
            "Page 172 complete. Extracted 20 products.\n",
            "Loading page 173...\n",
            "Page 173 complete. Extracted 20 products.\n",
            "Loading page 174...\n",
            "Page 174 complete. Extracted 20 products.\n",
            "Loading page 175...\n",
            "Page 175 complete. Extracted 20 products.\n",
            "Loading page 176...\n",
            "Page 176 complete. Extracted 20 products.\n",
            "Loading page 177...\n",
            "Page 177 complete. Extracted 20 products.\n",
            "Loading page 178...\n",
            "Page 178 complete. Extracted 20 products.\n",
            "Loading page 179...\n",
            "Page 179 complete. Extracted 20 products.\n",
            "Loading page 180...\n",
            "Page 180 complete. Extracted 20 products.\n",
            "Loading page 181...\n",
            "Page 181 complete. Extracted 20 products.\n",
            "Loading page 182...\n",
            "Page 182 complete. Extracted 20 products.\n",
            "Loading page 183...\n",
            "Page 183 complete. Extracted 20 products.\n",
            "Loading page 184...\n",
            "Page 184 complete. Extracted 20 products.\n",
            "Loading page 185...\n",
            "Page 185 complete. Extracted 20 products.\n",
            "Loading page 186...\n",
            "Page 186 complete. Extracted 20 products.\n",
            "Loading page 187...\n",
            "Page 187 complete. Extracted 20 products.\n",
            "Loading page 188...\n",
            "Page 188 complete. Extracted 20 products.\n",
            "Loading page 189...\n",
            "Page 189 complete. Extracted 20 products.\n",
            "Loading page 190...\n",
            "Page 190 complete. Extracted 20 products.\n",
            "Loading page 191...\n",
            "Page 191 complete. Extracted 20 products.\n",
            "Loading page 192...\n",
            "Page 192 complete. Extracted 20 products.\n",
            "Loading page 193...\n",
            "Page 193 complete. Extracted 20 products.\n",
            "Loading page 194...\n",
            "Page 194 complete. Extracted 20 products.\n",
            "Loading page 195...\n",
            "Page 195 complete. Extracted 20 products.\n",
            "Loading page 196...\n",
            "Page 196 complete. Extracted 20 products.\n",
            "Loading page 197...\n",
            "Page 197 complete. Extracted 20 products.\n",
            "Loading page 198...\n",
            "Page 198 complete. Extracted 20 products.\n",
            "Loading page 199...\n",
            "Page 199 complete. Extracted 20 products.\n",
            "Loading page 200...\n",
            "Page 200 complete. Extracted 20 products.\n",
            "Loading page 201...\n",
            "Page 201 complete. Extracted 20 products.\n",
            "Loading page 202...\n",
            "Page 202 complete. Extracted 20 products.\n",
            "Loading page 203...\n",
            "Page 203 complete. Extracted 20 products.\n",
            "Loading page 204...\n",
            "Page 204 complete. Extracted 20 products.\n",
            "Loading page 205...\n",
            "Page 205 complete. Extracted 20 products.\n",
            "Loading page 206...\n",
            "Page 206 complete. Extracted 20 products.\n",
            "Loading page 207...\n",
            "Page 207 complete. Extracted 20 products.\n",
            "Loading page 208...\n",
            "Page 208 complete. Extracted 20 products.\n",
            "Loading page 209...\n",
            "Page 209 complete. Extracted 20 products.\n",
            "Loading page 210...\n",
            "Page 210 complete. Extracted 20 products.\n",
            "Loading page 211...\n",
            "Page 211 complete. Extracted 20 products.\n",
            "Loading page 212...\n",
            "Page 212 complete. Extracted 20 products.\n",
            "Loading page 213...\n",
            "Page 213 complete. Extracted 20 products.\n",
            "Loading page 214...\n",
            "Page 214 complete. Extracted 20 products.\n",
            "Loading page 215...\n",
            "Page 215 complete. Extracted 20 products.\n",
            "Loading page 216...\n",
            "Page 216 complete. Extracted 20 products.\n",
            "Loading page 217...\n",
            "Page 217 complete. Extracted 20 products.\n",
            "Loading page 218...\n",
            "Page 218 complete. Extracted 20 products.\n",
            "Loading page 219...\n",
            "Page 219 complete. Extracted 20 products.\n",
            "Loading page 220...\n",
            "Page 220 complete. Extracted 20 products.\n",
            "Loading page 221...\n",
            "Page 221 complete. Extracted 20 products.\n",
            "Loading page 222...\n",
            "Page 222 complete. Extracted 20 products.\n",
            "Loading page 223...\n",
            "Page 223 complete. Extracted 20 products.\n",
            "Loading page 224...\n",
            "Page 224 complete. Extracted 20 products.\n",
            "Loading page 225...\n",
            "Page 225 complete. Extracted 20 products.\n",
            "Loading page 226...\n",
            "Page 226 complete. Extracted 20 products.\n",
            "Loading page 227...\n",
            "Page 227 complete. Extracted 20 products.\n",
            "Loading page 228...\n",
            "Page 228 complete. Extracted 20 products.\n",
            "Loading page 229...\n",
            "Page 229 complete. Extracted 20 products.\n",
            "Loading page 230...\n",
            "Page 230 complete. Extracted 20 products.\n",
            "Loading page 231...\n",
            "Page 231 complete. Extracted 20 products.\n",
            "Loading page 232...\n",
            "Page 232 complete. Extracted 20 products.\n",
            "Loading page 233...\n",
            "Page 233 complete. Extracted 20 products.\n",
            "Loading page 234...\n",
            "Page 234 complete. Extracted 20 products.\n",
            "Loading page 235...\n",
            "Page 235 complete. Extracted 20 products.\n",
            "Loading page 236...\n",
            "Page 236 complete. Extracted 20 products.\n",
            "Loading page 237...\n",
            "Page 237 complete. Extracted 20 products.\n",
            "Loading page 238...\n",
            "Page 238 complete. Extracted 20 products.\n",
            "Loading page 239...\n",
            "Page 239 complete. Extracted 20 products.\n",
            "Loading page 240...\n",
            "Page 240 complete. Extracted 20 products.\n",
            "Loading page 241...\n",
            "Page 241 complete. Extracted 20 products.\n",
            "Loading page 242...\n",
            "Page 242 complete. Extracted 20 products.\n",
            "Loading page 243...\n",
            "Page 243 complete. Extracted 20 products.\n",
            "Loading page 244...\n",
            "Page 244 complete. Extracted 20 products.\n",
            "Loading page 245...\n",
            "Page 245 complete. Extracted 20 products.\n",
            "Loading page 246...\n",
            "Page 246 complete. Extracted 20 products.\n",
            "Loading page 247...\n",
            "Page 247 complete. Extracted 20 products.\n",
            "Loading page 248...\n",
            "Page 248 complete. Extracted 20 products.\n",
            "Loading page 249...\n",
            "Page 249 complete. Extracted 20 products.\n",
            "Loading page 250...\n",
            "Page 250 complete. Extracted 20 products.\n",
            "Loading page 251...\n",
            "Page 251 complete. Extracted 20 products.\n",
            "Loading page 252...\n",
            "Page 252 complete. Extracted 20 products.\n",
            "Loading page 253...\n",
            "Page 253 complete. Extracted 20 products.\n",
            "Loading page 254...\n",
            "Page 254 complete. Extracted 20 products.\n",
            "Loading page 255...\n",
            "Page 255 complete. Extracted 20 products.\n",
            "Loading page 256...\n",
            "Page 256 complete. Extracted 20 products.\n",
            "Loading page 257...\n",
            "Page 257 complete. Extracted 20 products.\n",
            "Loading page 258...\n",
            "Page 258 complete. Extracted 20 products.\n",
            "Loading page 259...\n",
            "Page 259 complete. Extracted 20 products.\n",
            "Loading page 260...\n",
            "Page 260 complete. Extracted 20 products.\n",
            "Loading page 261...\n",
            "Page 261 complete. Extracted 20 products.\n",
            "Loading page 262...\n",
            "Page 262 complete. Extracted 20 products.\n",
            "Loading page 263...\n",
            "Page 263 complete. Extracted 20 products.\n",
            "Loading page 264...\n",
            "Page 264 complete. Extracted 20 products.\n",
            "Loading page 265...\n",
            "Page 265 complete. Extracted 20 products.\n",
            "Loading page 266...\n",
            "Page 266 complete. Extracted 20 products.\n",
            "Loading page 267...\n",
            "Page 267 complete. Extracted 20 products.\n",
            "Loading page 268...\n",
            "Page 268 complete. Extracted 20 products.\n",
            "Loading page 269...\n",
            "Page 269 complete. Extracted 20 products.\n",
            "Loading page 270...\n",
            "Page 270 complete. Extracted 20 products.\n",
            "Loading page 271...\n",
            "Page 271 complete. Extracted 20 products.\n",
            "Loading page 272...\n",
            "Page 272 complete. Extracted 20 products.\n",
            "Loading page 273...\n",
            "Page 273 complete. Extracted 20 products.\n",
            "Loading page 274...\n",
            "Page 274 complete. Extracted 20 products.\n",
            "Loading page 275...\n",
            "Page 275 complete. Extracted 20 products.\n",
            "Loading page 276...\n",
            "Page 276 complete. Extracted 20 products.\n",
            "Loading page 277...\n",
            "Page 277 complete. Extracted 20 products.\n",
            "Loading page 278...\n",
            "Page 278 complete. Extracted 20 products.\n",
            "Loading page 279...\n",
            "Page 279 complete. Extracted 20 products.\n",
            "Loading page 280...\n",
            "Page 280 complete. Extracted 20 products.\n",
            "Loading page 281...\n",
            "Page 281 complete. Extracted 20 products.\n",
            "Loading page 282...\n",
            "Page 282 complete. Extracted 20 products.\n",
            "Loading page 283...\n",
            "Page 283 complete. Extracted 20 products.\n",
            "Loading page 284...\n",
            "Page 284 complete. Extracted 20 products.\n",
            "Loading page 285...\n",
            "Page 285 complete. Extracted 20 products.\n",
            "Loading page 286...\n",
            "Page 286 complete. Extracted 20 products.\n",
            "Loading page 287...\n",
            "Page 287 complete. Extracted 20 products.\n",
            "Loading page 288...\n",
            "Page 288 complete. Extracted 20 products.\n",
            "Loading page 289...\n",
            "Page 289 complete. Extracted 20 products.\n",
            "Loading page 290...\n",
            "Page 290 complete. Extracted 20 products.\n",
            "Loading page 291...\n",
            "Page 291 complete. Extracted 20 products.\n",
            "Loading page 292...\n",
            "Page 292 complete. Extracted 20 products.\n",
            "Loading page 293...\n",
            "Page 293 complete. Extracted 20 products.\n",
            "Loading page 294...\n",
            "Page 294 complete. Extracted 20 products.\n",
            "Loading page 295...\n",
            "Page 295 complete. Extracted 20 products.\n",
            "Loading page 296...\n",
            "Page 296 complete. Extracted 20 products.\n",
            "Loading page 297...\n",
            "Page 297 complete. Extracted 20 products.\n",
            "Loading page 298...\n",
            "Page 298 complete. Extracted 20 products.\n",
            "Loading page 299...\n",
            "Page 299 complete. Extracted 20 products.\n",
            "Loading page 300...\n",
            "Page 300 complete. Extracted 20 products.\n",
            "Loading page 301...\n",
            "Page 301 complete. Extracted 20 products.\n",
            "Loading page 302...\n",
            "Page 302 complete. Extracted 20 products.\n",
            "Loading page 303...\n",
            "Page 303 complete. Extracted 20 products.\n",
            "Loading page 304...\n",
            "Page 304 complete. Extracted 20 products.\n",
            "Loading page 305...\n",
            "Page 305 complete. Extracted 20 products.\n",
            "Loading page 306...\n",
            "Page 306 complete. Extracted 20 products.\n",
            "Loading page 307...\n",
            "Page 307 complete. Extracted 20 products.\n",
            "Loading page 308...\n",
            "Skipping page 308 due to error.\n",
            "Loading page 309...\n",
            "Page 309 complete. Extracted 20 products.\n",
            "Loading page 310...\n",
            "Page 310 complete. Extracted 20 products.\n",
            "Loading page 311...\n",
            "Page 311 complete. Extracted 20 products.\n",
            "Loading page 312...\n",
            "Page 312 complete. Extracted 20 products.\n",
            "Loading page 313...\n",
            "Page 313 complete. Extracted 20 products.\n",
            "Loading page 314...\n",
            "Page 314 complete. Extracted 20 products.\n",
            "Loading page 315...\n",
            "Page 315 complete. Extracted 20 products.\n",
            "Loading page 316...\n",
            "Page 316 complete. Extracted 20 products.\n",
            "Loading page 317...\n",
            "Page 317 complete. Extracted 20 products.\n",
            "Loading page 318...\n",
            "Page 318 complete. Extracted 20 products.\n",
            "Loading page 319...\n",
            "Page 319 complete. Extracted 20 products.\n",
            "Loading page 320...\n",
            "Page 320 complete. Extracted 20 products.\n",
            "Loading page 321...\n",
            "Page 321 complete. Extracted 20 products.\n",
            "Loading page 322...\n",
            "Page 322 complete. Extracted 20 products.\n",
            "Loading page 323...\n",
            "Page 323 complete. Extracted 20 products.\n",
            "Loading page 324...\n",
            "Page 324 complete. Extracted 20 products.\n",
            "Loading page 325...\n",
            "Page 325 complete. Extracted 20 products.\n",
            "Loading page 326...\n",
            "Page 326 complete. Extracted 20 products.\n",
            "Loading page 327...\n",
            "Page 327 complete. Extracted 20 products.\n",
            "Loading page 328...\n",
            "Page 328 complete. Extracted 20 products.\n",
            "Loading page 329...\n",
            "Page 329 complete. Extracted 20 products.\n",
            "Loading page 330...\n",
            "Page 330 complete. Extracted 20 products.\n",
            "Loading page 331...\n",
            "Page 331 complete. Extracted 20 products.\n",
            "Loading page 332...\n",
            "Page 332 complete. Extracted 20 products.\n",
            "Loading page 333...\n",
            "Page 333 complete. Extracted 20 products.\n",
            "Loading page 334...\n",
            "Page 334 complete. Extracted 20 products.\n",
            "Loading page 335...\n",
            "Page 335 complete. Extracted 20 products.\n",
            "Loading page 336...\n",
            "Page 336 complete. Extracted 20 products.\n",
            "Loading page 337...\n",
            "Page 337 complete. Extracted 20 products.\n",
            "Loading page 338...\n",
            "Page 338 complete. Extracted 20 products.\n",
            "Loading page 339...\n",
            "Page 339 complete. Extracted 20 products.\n",
            "Loading page 340...\n",
            "Page 340 complete. Extracted 20 products.\n",
            "Loading page 341...\n",
            "Page 341 complete. Extracted 20 products.\n",
            "Loading page 342...\n",
            "Page 342 complete. Extracted 20 products.\n",
            "Loading page 343...\n",
            "Page 343 complete. Extracted 20 products.\n",
            "Loading page 344...\n",
            "Page 344 complete. Extracted 20 products.\n",
            "Loading page 345...\n",
            "Page 345 complete. Extracted 20 products.\n",
            "Loading page 346...\n",
            "Page 346 complete. Extracted 20 products.\n",
            "Loading page 347...\n",
            "Page 347 complete. Extracted 20 products.\n",
            "Loading page 348...\n",
            "Page 348 complete. Extracted 20 products.\n",
            "Loading page 349...\n",
            "Page 349 complete. Extracted 20 products.\n",
            "Loading page 350...\n",
            "Page 350 complete. Extracted 20 products.\n",
            "Loading page 351...\n",
            "Page 351 complete. Extracted 20 products.\n",
            "Loading page 352...\n",
            "Page 352 complete. Extracted 20 products.\n",
            "Loading page 353...\n",
            "Page 353 complete. Extracted 20 products.\n",
            "Loading page 354...\n",
            "Page 354 complete. Extracted 20 products.\n",
            "Loading page 355...\n",
            "Page 355 complete. Extracted 20 products.\n",
            "Loading page 356...\n",
            "Page 356 complete. Extracted 20 products.\n",
            "Loading page 357...\n",
            "Page 357 complete. Extracted 20 products.\n",
            "Loading page 358...\n",
            "Page 358 complete. Extracted 20 products.\n",
            "Loading page 359...\n",
            "Page 359 complete. Extracted 20 products.\n",
            "Loading page 360...\n",
            "Page 360 complete. Extracted 20 products.\n",
            "Loading page 361...\n",
            "Page 361 complete. Extracted 20 products.\n",
            "Loading page 362...\n",
            "Page 362 complete. Extracted 20 products.\n",
            "Loading page 363...\n",
            "Page 363 complete. Extracted 20 products.\n",
            "Loading page 364...\n",
            "Page 364 complete. Extracted 20 products.\n",
            "Loading page 365...\n",
            "Page 365 complete. Extracted 20 products.\n",
            "Loading page 366...\n",
            "Skipping page 366 due to error.\n",
            "Loading page 367...\n",
            "Skipping page 367 due to error.\n",
            "Loading page 368...\n",
            "Page 368 complete. Extracted 20 products.\n",
            "Loading page 369...\n",
            "Page 369 complete. Extracted 20 products.\n",
            "Loading page 370...\n",
            "Page 370 complete. Extracted 20 products.\n",
            "Loading page 371...\n",
            "Page 371 complete. Extracted 20 products.\n",
            "Loading page 372...\n",
            "Page 372 complete. Extracted 20 products.\n",
            "Loading page 373...\n",
            "Page 373 complete. Extracted 20 products.\n",
            "Loading page 374...\n",
            "Page 374 complete. Extracted 20 products.\n",
            "Loading page 375...\n",
            "Page 375 complete. Extracted 20 products.\n",
            "Loading page 376...\n",
            "Page 376 complete. Extracted 20 products.\n",
            "Loading page 377...\n",
            "Page 377 complete. Extracted 20 products.\n",
            "Loading page 378...\n",
            "Page 378 complete. Extracted 20 products.\n",
            "Loading page 379...\n",
            "Page 379 complete. Extracted 20 products.\n",
            "Loading page 380...\n",
            "Page 380 complete. Extracted 20 products.\n",
            "Loading page 381...\n",
            "Page 381 complete. Extracted 20 products.\n",
            "Loading page 382...\n",
            "Page 382 complete. Extracted 20 products.\n",
            "Loading page 383...\n",
            "Page 383 complete. Extracted 20 products.\n",
            "Loading page 384...\n",
            "Page 384 complete. Extracted 20 products.\n",
            "Loading page 385...\n",
            "Page 385 complete. Extracted 20 products.\n",
            "Loading page 386...\n",
            "Page 386 complete. Extracted 20 products.\n",
            "Loading page 387...\n",
            "Page 387 complete. Extracted 20 products.\n",
            "Loading page 388...\n",
            "Page 388 complete. Extracted 20 products.\n",
            "Loading page 389...\n",
            "Page 389 complete. Extracted 20 products.\n",
            "Loading page 390...\n",
            "Page 390 complete. Extracted 20 products.\n",
            "Loading page 391...\n",
            "Page 391 complete. Extracted 20 products.\n",
            "Loading page 392...\n",
            "Page 392 complete. Extracted 20 products.\n",
            "Loading page 393...\n",
            "Page 393 complete. Extracted 20 products.\n",
            "Loading page 394...\n",
            "Page 394 complete. Extracted 20 products.\n",
            "Loading page 395...\n",
            "Page 395 complete. Extracted 20 products.\n",
            "Loading page 396...\n",
            "Page 396 complete. Extracted 20 products.\n",
            "Loading page 397...\n",
            "Page 397 complete. Extracted 20 products.\n",
            "Loading page 398...\n",
            "Page 398 complete. Extracted 20 products.\n",
            "Loading page 399...\n",
            "Page 399 complete. Extracted 20 products.\n",
            "Loading page 400...\n",
            "Page 400 complete. Extracted 20 products.\n",
            "Loading page 401...\n",
            "Page 401 complete. Extracted 20 products.\n",
            "Loading page 402...\n",
            "Page 402 complete. Extracted 20 products.\n",
            "Loading page 403...\n",
            "Page 403 complete. Extracted 20 products.\n",
            "Loading page 404...\n",
            "Page 404 complete. Extracted 20 products.\n",
            "Loading page 405...\n",
            "Page 405 complete. Extracted 20 products.\n",
            "Loading page 406...\n",
            "Page 406 complete. Extracted 20 products.\n",
            "Loading page 407...\n",
            "Page 407 complete. Extracted 20 products.\n",
            "Loading page 408...\n",
            "Page 408 complete. Extracted 20 products.\n",
            "Loading page 409...\n",
            "Page 409 complete. Extracted 20 products.\n",
            "Loading page 410...\n",
            "Page 410 complete. Extracted 20 products.\n",
            "Loading page 411...\n",
            "Page 411 complete. Extracted 20 products.\n",
            "Loading page 412...\n",
            "Page 412 complete. Extracted 20 products.\n",
            "Loading page 413...\n",
            "Page 413 complete. Extracted 20 products.\n",
            "Loading page 414...\n",
            "Page 414 complete. Extracted 20 products.\n",
            "Loading page 415...\n",
            "Page 415 complete. Extracted 20 products.\n",
            "Loading page 416...\n",
            "Page 416 complete. Extracted 20 products.\n",
            "Loading page 417...\n",
            "Page 417 complete. Extracted 20 products.\n",
            "Loading page 418...\n",
            "Page 418 complete. Extracted 20 products.\n",
            "Loading page 419...\n",
            "Page 419 complete. Extracted 20 products.\n",
            "Loading page 420...\n",
            "Page 420 complete. Extracted 20 products.\n",
            "Loading page 421...\n",
            "Page 421 complete. Extracted 20 products.\n",
            "Loading page 422...\n",
            "Page 422 complete. Extracted 20 products.\n",
            "Loading page 423...\n",
            "Page 423 complete. Extracted 20 products.\n",
            "Loading page 424...\n",
            "Page 424 complete. Extracted 20 products.\n",
            "Loading page 425...\n",
            "Page 425 complete. Extracted 20 products.\n",
            "Loading page 426...\n",
            "Page 426 complete. Extracted 20 products.\n",
            "Loading page 427...\n",
            "Page 427 complete. Extracted 20 products.\n",
            "Loading page 428...\n",
            "Page 428 complete. Extracted 20 products.\n",
            "Loading page 429...\n",
            "Page 429 complete. Extracted 20 products.\n",
            "Loading page 430...\n",
            "Page 430 complete. Extracted 20 products.\n",
            "Loading page 431...\n",
            "Page 431 complete. Extracted 20 products.\n",
            "Loading page 432...\n",
            "Page 432 complete. Extracted 20 products.\n",
            "Loading page 433...\n",
            "Page 433 complete. Extracted 20 products.\n",
            "Loading page 434...\n",
            "Page 434 complete. Extracted 20 products.\n",
            "Loading page 435...\n",
            "Page 435 complete. Extracted 20 products.\n",
            "Loading page 436...\n",
            "Page 436 complete. Extracted 20 products.\n",
            "Loading page 437...\n",
            "Page 437 complete. Extracted 20 products.\n",
            "Loading page 438...\n",
            "Page 438 complete. Extracted 20 products.\n",
            "Loading page 439...\n",
            "Page 439 complete. Extracted 20 products.\n",
            "Loading page 440...\n",
            "Page 440 complete. Extracted 20 products.\n",
            "Loading page 441...\n",
            "Page 441 complete. Extracted 20 products.\n",
            "Loading page 442...\n",
            "Page 442 complete. Extracted 20 products.\n",
            "Loading page 443...\n",
            "Page 443 complete. Extracted 20 products.\n",
            "Loading page 444...\n",
            "Page 444 complete. Extracted 20 products.\n",
            "Loading page 445...\n",
            "Page 445 complete. Extracted 20 products.\n",
            "Loading page 446...\n",
            "Page 446 complete. Extracted 20 products.\n",
            "Loading page 447...\n",
            "Page 447 complete. Extracted 20 products.\n",
            "Loading page 448...\n",
            "Page 448 complete. Extracted 20 products.\n",
            "Loading page 449...\n",
            "Page 449 complete. Extracted 20 products.\n",
            "Loading page 450...\n",
            "Page 450 complete. Extracted 20 products.\n",
            "Loading page 451...\n",
            "Page 451 complete. Extracted 20 products.\n",
            "Loading page 452...\n",
            "Page 452 complete. Extracted 20 products.\n",
            "Loading page 453...\n",
            "Page 453 complete. Extracted 20 products.\n",
            "Loading page 454...\n",
            "Page 454 complete. Extracted 20 products.\n",
            "Loading page 455...\n",
            "Page 455 complete. Extracted 20 products.\n",
            "Loading page 456...\n",
            "Page 456 complete. Extracted 20 products.\n",
            "Loading page 457...\n",
            "Page 457 complete. Extracted 20 products.\n",
            "Loading page 458...\n",
            "Page 458 complete. Extracted 20 products.\n",
            "Loading page 459...\n",
            "Page 459 complete. Extracted 20 products.\n",
            "Loading page 460...\n",
            "Page 460 complete. Extracted 20 products.\n",
            "Loading page 461...\n",
            "Page 461 complete. Extracted 20 products.\n",
            "Loading page 462...\n",
            "Page 462 complete. Extracted 20 products.\n",
            "Loading page 463...\n",
            "Page 463 complete. Extracted 20 products.\n",
            "Loading page 464...\n",
            "Page 464 complete. Extracted 20 products.\n",
            "Loading page 465...\n",
            "Page 465 complete. Extracted 20 products.\n",
            "Loading page 466...\n",
            "Page 466 complete. Extracted 20 products.\n",
            "Loading page 467...\n",
            "Page 467 complete. Extracted 20 products.\n",
            "Loading page 468...\n",
            "Page 468 complete. Extracted 20 products.\n",
            "Loading page 469...\n",
            "Page 469 complete. Extracted 20 products.\n",
            "Loading page 470...\n",
            "Page 470 complete. Extracted 20 products.\n",
            "Loading page 471...\n",
            "Page 471 complete. Extracted 20 products.\n",
            "Loading page 472...\n",
            "Page 472 complete. Extracted 20 products.\n",
            "Loading page 473...\n",
            "Page 473 complete. Extracted 20 products.\n",
            "Loading page 474...\n",
            "Page 474 complete. Extracted 20 products.\n",
            "Loading page 475...\n",
            "Page 475 complete. Extracted 20 products.\n",
            "Loading page 476...\n",
            "Page 476 complete. Extracted 20 products.\n",
            "Loading page 477...\n",
            "Page 477 complete. Extracted 20 products.\n",
            "Loading page 478...\n",
            "Page 478 complete. Extracted 20 products.\n",
            "Loading page 479...\n",
            "Page 479 complete. Extracted 20 products.\n",
            "Loading page 480...\n",
            "Page 480 complete. Extracted 20 products.\n",
            "Loading page 481...\n",
            "Page 481 complete. Extracted 20 products.\n",
            "Loading page 482...\n",
            "Page 482 complete. Extracted 20 products.\n",
            "Loading page 483...\n",
            "Page 483 complete. Extracted 20 products.\n",
            "Loading page 484...\n",
            "Page 484 complete. Extracted 20 products.\n",
            "Loading page 485...\n",
            "Page 485 complete. Extracted 20 products.\n",
            "Loading page 486...\n",
            "Page 486 complete. Extracted 20 products.\n",
            "Loading page 487...\n",
            "Page 487 complete. Extracted 20 products.\n",
            "Loading page 488...\n",
            "Page 488 complete. Extracted 20 products.\n",
            "Loading page 489...\n",
            "Page 489 complete. Extracted 20 products.\n",
            "Loading page 490...\n",
            "Page 490 complete. Extracted 20 products.\n",
            "Loading page 491...\n",
            "Page 491 complete. Extracted 20 products.\n",
            "Loading page 492...\n",
            "Page 492 complete. Extracted 20 products.\n",
            "Loading page 493...\n",
            "Page 493 complete. Extracted 20 products.\n",
            "Loading page 494...\n",
            "Page 494 complete. Extracted 20 products.\n",
            "Loading page 495...\n",
            "Page 495 complete. Extracted 20 products.\n",
            "Loading page 496...\n",
            "Page 496 complete. Extracted 20 products.\n",
            "Loading page 497...\n",
            "Page 497 complete. Extracted 20 products.\n",
            "Loading page 498...\n",
            "Page 498 complete. Extracted 20 products.\n",
            "Loading page 499...\n",
            "Page 499 complete. Extracted 20 products.\n",
            "Loading page 500...\n",
            "Page 500 complete. Extracted 20 products.\n",
            "Product data saved to 'scraped_products.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part 2\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Read the CSV data\n",
        "data = pd.read_csv(\"scraped_products.csv\")\n",
        "print(\"Original Data Loaded.\")\n",
        "\n",
        "# -------------------------------\n",
        "# Data Quality Operations\n",
        "# -------------------------------\n",
        "\n",
        "# Remove duplicate rows\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Drop rows where critical columns are missing\n",
        "data.dropna(subset=[\"Product Name\", \"Description\"], inplace=True)\n",
        "\n",
        "# Reset index after cleaning\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Text Preprocessing Functions\n",
        "# -------------------------------\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\" Remove HTML tags, special characters, and digits, then convert text to lowercase. \"\"\"\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabet characters\n",
        "    return text.lower().strip()  # Convert to lowercase and strip whitespace\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\" Preprocess text by cleaning, tokenizing, removing stopwords, and lemmatizing. \"\"\"\n",
        "    text = clean_text(text)\n",
        "    tokens = word_tokenize(text)  # Tokenization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Stopword removal & Lemmatization\n",
        "    return \" \".join(tokens)  # Join tokens back into a string\n",
        "\n",
        "# -------------------------------\n",
        "# Apply Preprocessing to Columns\n",
        "# -------------------------------\n",
        "\n",
        "data[\"Product Name Processed\"] = data[\"Product Name\"].astype(str).apply(preprocess_text)\n",
        "data[\"Description Processed\"] = data[\"Description\"].astype(str).apply(preprocess_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "output_filename = \"cleaned_github_marketplace_data.csv\"\n",
        "data.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Cleaned data saved as: {output_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sm5OOAygwdX",
        "outputId": "3acfdcda-0b8b-44fa-e37a-9004d3ead810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Loaded.\n",
            "Cleaned data saved as: cleaned_github_marketplace_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"cleaned_github_marketplace_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0UynqkBgkMqx",
        "outputId": "6777b315-6c9c-46d5-e3a0-656f99662c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fac1a4b9-7797-4035-82b7-376e60169fc2\", \"cleaned_github_marketplace_data.csv\", 2117037)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# PART 1: DATA EXTRACTION\n",
        "# Set  Bearer Token\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAOOCzQEAAAAAQWYXvw9dFI3QIBYBfV0hFb11qi8%3DI5UeP4qR3zA6YRODaVnccyXG70dNCNIvk5rfSC120GZ0krweWz\"\n",
        "\n",
        "# Create a Tweepy client using the bearer token\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Define the query to search for tweets that include the hashtags for machine learning or artificial intelligence.\n",
        "# Also, exclude retweets and limit results to English-language tweets.\n",
        "query = \"#machinelearning OR #artificialintelligence -is:retweet lang:en\"\n",
        "\n",
        "# Request tweet fields and expand the author details to get the username.\n",
        "response = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    tweet_fields=[\"id\", \"text\", \"author_id\"],\n",
        "    expansions=\"author_id\",\n",
        "    user_fields=[\"username\"],\n",
        "    max_results=100  # Maximum allowed per request\n",
        ")\n",
        "\n",
        "# Create a mapping of user IDs to their corresponding usernames\n",
        "tweets_data = []\n",
        "if response.data and response.includes and \"users\" in response.includes:\n",
        "    users = {u[\"id\"]: u for u in response.includes[\"users\"]}\n",
        "    for tweet in response.data:\n",
        "        user_info = users.get(tweet.author_id)\n",
        "        if user_info:  # Ensure we have user details\n",
        "            tweets_data.append({\n",
        "                \"tweet_id\": tweet.id,\n",
        "                \"username\": user_info.username,\n",
        "                \"text\": tweet.text\n",
        "            })\n",
        "\n",
        "print(f\"Extracted {len(tweets_data)} tweets.\")\n",
        "\n",
        "# PART 2: DATA CLEANING & QUALITY CHECK\n",
        "\n",
        "# Convert the extracted data to a DataFrame\n",
        "df = pd.DataFrame(tweets_data)\n",
        "\n",
        "# Define a function to clean tweet text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)       # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)          # Remove user mentions\n",
        "    text = re.sub(r\"#\", \"\", text)             # Remove hashtag symbol\n",
        "    text = re.sub(r\"\\s+\", \" \", text)          # Remove extra whitespace\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning to the tweet text and add as a new column\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Final quality check: remove any rows with missing or empty values in critical columns.\n",
        "df.dropna(subset=[\"tweet_id\", \"username\", \"clean_text\"], inplace=True)\n",
        "df = df[(df[\"tweet_id\"] != \"\") & (df[\"username\"] != \"\") & (df[\"clean_text\"] != \"\")]\n",
        "\n",
        "# Save the cleaned data to a CSV file for further analysis.\n",
        "csv_filename = \"cleaned_tweets.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f\"Cleaned data saved to {csv_filename}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k2DB8KHOkpn",
        "outputId": "249bfbb8-8e3c-4120-b345-621b27801628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n",
            "Extracted 99 tweets.\n",
            "Cleaned data saved to cleaned_tweets.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"cleaned_tweets.csv\")"
      ],
      "metadata": {
        "id": "eJzeMyCCPhJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e5bec43a-d94b-4c49-af4d-e6c99bc2caf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b2acea5f-300e-4415-a109-d9f3b3c4d3f6\", \"cleaned_tweets.csv\", 39802)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below:\n",
        "The assignment was but but I found some question challenging like scrapping products from github market place. I enjoyed scrapping data though. it felt like hacking! It took me 12hrs to complete the assignment.\n",
        "\n",
        "\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VN42A8_lk0_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}